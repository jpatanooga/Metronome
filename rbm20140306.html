<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print" />

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Restricted Boltzmann Machine Visualizations</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>RBM Visualizations</h1>
        <h2>Renders of RBM Activations, Weight Histograms, and Learned Filters</h2>
        <a href="https://github.com/jpatanooga/Metronome" class="button"><small>View project on</small>GitHub</a>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h3>

<a name="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages"><span class="octicon octicon-link"></span></a>Renders from March 06, 2014.</h3>

<p>Something interesting that Hinton has proposed is that <b>we can take a look about what machines "dream about"</b> in their sleep state.</p>

<p>
In the spirit of reproducing good research to baseline our implementation, we've visualiezed our RBMs based on the techniques from Yosinski and Lipson's paper:
</p>

</p>
"Visually Debugging Restricted Boltzmann Machine Training with a 3D Example"

<a href="http://yosinski.com/media/papers/Yosinski2012VisuallyDebuggingRestrictedBoltzmannMachine.pdf">http://yosinski.com/media/papers/Yosinski2012VisuallyDebuggingRestrictedBoltzmannMachine.pdf</a>
</p>

<p>
Below is a sampling from the renders taken from our Deep Learning implementation of Restricted Boltzmann Machines in Metronome. In these renders the RBMs are learning reprensetations of the canonical <a href="http://yann.lecun.com/exdb/mnist/">MNIST Dataset</a>. As you can see in the learned filter, portions of digits are clearly visible.
</p>

<h4>Filter Renders</h4>

<blockquote> ...we plot the learned filter for each hidden neuron, one per column of W. Each filter is of the same dimension as the input data, and it is most useful to visualize the filters in the same way
as the input data is visualized. In the cases of image patches, we show each filter as an image patch...</blockquote>

<p>

<table>
<tr>
<td>
Starting state for filters
</td>
<td>
Filters at 206 Cross Entropy
</td>
<td>
Filters at 4.9 Cross Entropy
</td>
<tr>

<tr>
<td>
<img src="https://raw.github.com/jpatanooga/jpatanooga.github.io/master/images/deeplearning/rbm20140306/filters_init_ce.png" />
</td>
<td>
<img src="https://raw.github.com/jpatanooga/jpatanooga.github.io/master/images/deeplearning/rbm20140306/filters_206.7_ce.png" />
</td>
<td>
<img src="https://raw.github.com/jpatanooga/jpatanooga.github.io/master/images/deeplearning/rbm20140306/filters_4.996_ce.png" />
</td>
</tr>
</table>

</p>

<h4>Activation Renders</h4>

<blockquote>...we plot this probability of activation for each hidden neuron...</blockquote>


<p>
At Initialization Time:
</p>
<p>
<img src="https://raw.github.com/jpatanooga/jpatanooga.github.io/master/images/deeplearning/rbm20140306/activations_init_ce.png" />
</p>

<p>
At 206 Cross Entropy:
</p>
<p>
<img src="https://raw.github.com/jpatanooga/jpatanooga.github.io/master/images/deeplearning/rbm20140306/activations_206.7_ce.png" />
</p>

<p>
At 78 Cross Entropy:
</p>
<p>
<img src="https://raw.github.com/jpatanooga/jpatanooga.github.io/master/images/deeplearning/rbm20140306/activations_78.52_ce.png" />
</p>

<p>
At 4.9 Cross Entropy:
</p>
<p>
<img src="https://raw.github.com/jpatanooga/jpatanooga.github.io/master/images/deeplearning/rbm20140306/activations_4.996_ce.png" />
</p>

<h4>Weight Histogram Renders</h4>

<p>A look at how the connection weights in the RBMs change during training in the form of a weight histogram.</p>


<p>
At Initialization Time:
</p>

<p>
<img src="https://raw.github.com/jpatanooga/jpatanooga.github.io/master/images/deeplearning/rbm20140306/weight_histogram_init_ce.png" />

</p>

<p>
At 4.9 Cross Entropy:
</p>
<p>
<img src="https://raw.github.com/jpatanooga/jpatanooga.github.io/master/images/deeplearning/rbm20140306/weight_histogram_4.996_ce.png" />

</p>
      </div>
    </div>

  
  </body>
</html>
